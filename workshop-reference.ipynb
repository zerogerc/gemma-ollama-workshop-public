{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Getting started\n",
    "\n",
    "*Choose either Ollama or Google AI Studio setup*\n",
    "\n",
    "## Ollama (Fully Local)\n",
    "\n",
    "* Go to [https://ollama.com/](https://ollama.com/) download the binary, and install it.\n",
    "* If prompted, install command line tools.\n",
    "* If you have trouble with installation, go to the [Ollama GitHub repository](https://github.com/ollama/ollama) or [documentation](https://ollama.readthedocs.io/en/quickstart/) for more specific and up-to-date instructions.\n",
    "\n",
    "\n",
    "### Downloading the Gemma model\n",
    "\n",
    "Depending on your hardware, download one of the following models (we recommend 12b as it is quite smart but much smaller and faster than 27b).\n",
    "For today's workshop we are using Q4 quantised models, with relatively small inputs, so we suggest the following VRAM requirements:\n",
    "```\n",
    "| Model                            | VRAM  |\n",
    "|:---------------------------------|-------|\n",
    "| **PetrosStav/gemma3-tools:4b**   | 8 GB  |\n",
    "| **PetrosStav/gemma3-tools:12b**  | 16 GB |\n",
    "| **PetrosStav/gemma3-tools:27b**  | 32 GB |\n",
    "```\n",
    "\n",
    "### Testing the Gemma model\n",
    "\n",
    "```\n",
    "ollama pull <model-name>\n",
    "ollama run <model-name>\n",
    "```\n",
    "\n",
    "Ask to tell a joke or something and see if you get a response. Voilà, everything works!\n",
    "\n",
    "## Google AI Studio (API-based)\n",
    "\n",
    "* Go to [https://aistudio.google.com/](https://aistudio.google.com/) and sign in with your Google Account.\n",
    "* Click “Get API key” in the top-right corner of the page.\n",
    "* Click “Create API key” in the top-right corner of the page and then choose any available project in the dialog that appears.\n",
    "* Once the key is generated, copy it and paste it into this notebook."
   ],
   "id": "b7c0924da3cbd254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Let's get coding\n",
    "\n",
    "Create a virtual environment (or not) and install required dependencies:"
   ],
   "id": "c37d468bd2a8b331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install openai mcp langfuse pydantic nest_asyncio",
   "id": "646b697ae7bb9e05"
  },
  {
   "cell_type": "code",
   "id": "b4c78c1f2137e280",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletionMessage, ChatCompletionMessageToolCall, ChatCompletionToolParam\n",
    "from openai.types.chat.chat_completion_message_tool_call import Function\n",
    "import subprocess\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "556369e637444742",
   "metadata": {},
   "source": [
    "Now let us initialize a client of your choice and test that it works:"
   ]
  },
  {
   "cell_type": "code",
   "id": "45c8fbd6826714af",
   "metadata": {},
   "source": [
    "### Local ollama setup, make sure you run ollama serve beforehand\n",
    "# MODEL_NAME = \"PetrosStav/gemma3-tools:12b\"\n",
    "# client = OpenAI(\n",
    "#     base_url='http://localhost:11434/v1',\n",
    "#     api_key='ollama',  # unused for ollama\n",
    "# )\n",
    "\n",
    "### If you were not able to install ollama, it is too slow or something else; you can continue with the workshop with the gemini model\n",
    "MODEL_NAME = \"gemini-2.0-flash\"\n",
    "client = OpenAI(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=os.environ[\"AI_STUDIO_KEY\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46f0a07cc2c71925",
   "metadata": {},
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the determinant of [[1,2],[3,4]]?\"}\n",
    "    ]\n",
    ").choices[0].message.content\n",
    "\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5be93bff",
   "metadata": {},
   "source": [
    "# Part 2 - Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e1005396fc7472b",
   "metadata": {},
   "source": [
    "SYSTEM_MESSAGE = \"\"\"You are a chatbot assisting the developer in day-to-day tasks.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": \"List files in ~/Dev\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(json.dumps(response.choices[0].message.model_dump(), indent=4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bcb094c8c52532d0",
   "metadata": {},
   "source": [
    "list_directory_tool_description = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'list_directory',\n",
    "        'description': 'List the files in a directory',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'directory': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The path to the directory',\n",
    "                },\n",
    "            },\n",
    "            'required': ['directory'],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a chatbot assisting the developer in day-to-day tasks.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": \"List files in ~/Dev?\"}\n",
    "    ],\n",
    "    tools=[list_directory_tool_description]\n",
    ")\n",
    "\n",
    "print(json.dumps(response.choices[0].message.model_dump(), indent=4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "130149ef",
   "metadata": {},
   "source": [
    "# Part 3 - CLI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22a840e38626b",
   "metadata": {},
   "source": [
    "## Some miscellaneous setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "cfa1638245cd5958",
   "metadata": {},
   "source": [
    "UNSAFE_COMMANDS = [\n",
    "    \"rm\",\n",
    "    \"rmdir\",\n",
    "    \"mv\",\n",
    "    \"cp\",\n",
    "    \"dd\",\n",
    "    \"del\",\n",
    "    \"sudo\",\n",
    "    \"su\",\n",
    "    \">\",\n",
    "    \">>\",\n",
    "    \"2>\",\n",
    "    \"2>>\",\n",
    "    \"chmod\",\n",
    "    \"chown\",\n",
    "    \"shutdown\",\n",
    "    \"reboot\",\n",
    "    \"curl\",\n",
    "    \"wget\",\n",
    "]\n",
    "\n",
    "\n",
    "def execute_command(command: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a shell command and return the output.\n",
    "\n",
    "    If the command contains any unsafe commands, ask for user approval first.\n",
    "\n",
    "    Args:\n",
    "        command: The shell command to execute\n",
    "\n",
    "    Returns:\n",
    "        The command output or an error message\n",
    "    \"\"\"\n",
    "    # Check if command contains any unsafe commands\n",
    "    needs_approval = False\n",
    "    for unsafe in UNSAFE_COMMANDS:\n",
    "        if unsafe in command.split():\n",
    "            needs_approval = True\n",
    "            break\n",
    "\n",
    "    if needs_approval:\n",
    "        approval = input(\n",
    "            f\"\\nWARNING: The command '{command}' contains potentially dangerous operations.\\nDo you want to proceed? (y/n): \"\n",
    "        ).lower()\n",
    "        if approval != \"y\":\n",
    "            return \"Command execution cancelled by user.\"\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30,\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout\n",
    "        else:\n",
    "            return f\"Command exited with code {result.returncode}, stderr:\\n{result.stderr}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Command timed out after 30 seconds.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error executing command: {str(e)}\"\n",
    "\n",
    "\n",
    "def try_to_parse_tool_call(\n",
    "    message: ChatCompletionMessage,\n",
    ") -> ChatCompletionMessageToolCall | None:\n",
    "    if not \"```tool_call\" in message.content:\n",
    "        return None\n",
    "\n",
    "    tool_call_match = re.search(\n",
    "        r\"```tool_call(.*?)```\", message.content, re.DOTALL\n",
    "    ) or re.search(r\"```tool_call(.*?)</tool_call>`\", message.content, re.DOTALL)\n",
    "\n",
    "    if not tool_call_match:\n",
    "        return None\n",
    "\n",
    "    tool_call_json = tool_call_match.group(1)\n",
    "    \n",
    "    try:\n",
    "        tool_call_data = json.loads(tool_call_json)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing tool call: {tool_call_json}\")\n",
    "        return None\n",
    "    \n",
    "    if \"parameters\" not in tool_call_data and \"arguments\" in tool_call_data:\n",
    "        tool_call_data[\"parameters\"] = tool_call_data[\"arguments\"]\n",
    "    \n",
    "    return ChatCompletionMessageToolCall(\n",
    "        id=\"random_id\",\n",
    "        type=\"function\",\n",
    "        function=Function(\n",
    "            name=tool_call_data[\"name\"],\n",
    "            arguments=tool_call_data[\"parameters\"],\n",
    "        ),\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d90d7cb0e6b48e6",
   "metadata": {},
   "source": [
    "## Writing tool description"
   ]
  },
  {
   "cell_type": "code",
   "id": "1abb9c4b4bc09723",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "command_tool_description: ChatCompletionToolParam = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"execute_command\",\n",
    "        \"description\": \"Execute a shell command and return the output\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"command\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The shell command to execute\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"command\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def convert_tool_call_to_command(tool_call: ChatCompletionMessageToolCall) -> str:\n",
    "    return \" \".join(json.loads(tool_call.function.arguments).values())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aafac889e5b20712",
   "metadata": {},
   "source": [
    "## Writing system message and stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "id": "b757d8df1cc52d47",
   "metadata": {},
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a CLI agent.\n",
    "Your job is to interpret user requests and complete them by executing appropriate shell commands.\n",
    "You can only interact with the system by calling the tool `execute_command`, which runs a shell command and returns the output.\n",
    "\n",
    "Instructions:\n",
    "- Translate the user's request into one or more shell commands.\n",
    "- For each shell command, call `execute_command`.\n",
    "- For each shell command call provide a short (1-2 sentences) description of why we are you calling it.\n",
    "- Once all necessary commands have been executed answer a user's query and finish with word `DONE`.\n",
    "- If you need to write something to a file, use the following syntax: `cat > filename.txt <<EOF\\ncontent\\nEOF`\n",
    "\n",
    "Examples of tasks:\n",
    "- If the user asks to clone a GitHub repository, call `execute_command` with a `git clone` command.\n",
    "- If the user wants to install packages, call `execute_command` with a `pip install` or appropriate package manager command.\n",
    "- If the task involves multiple steps (e.g., creating a directory and then initializing a Git repo inside it), call `execute_command` for each step.\n",
    "\"\"\"\n",
    "\n",
    "def is_done(message: ChatCompletionMessage) -> bool:\n",
    "    return not message.tool_calls and (message.content or \"\").strip().endswith(\"DONE\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98203ab18333cd84",
   "metadata": {},
   "source": [
    "## Agent loop using your tool descriptions and system message"
   ]
  },
  {
   "cell_type": "code",
   "id": "de09f9c341d993ca",
   "metadata": {},
   "source": [
    "class CLIAgent:\n",
    "    \"\"\"CLI Agent that uses LM to process and execute commands.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: str,\n",
    "            client: OpenAI,\n",
    "            tools: list[ChatCompletionToolParam],\n",
    "            system_message: str,\n",
    "            max_steps: int = 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CLI Agent.\n",
    "\n",
    "        Args:\n",
    "            model: The OpenAI model to use\n",
    "            client: The OpenAI client to use for chat completions\n",
    "            system_message: The system message to use for the agent\n",
    "            max_steps: Maximum number of steps in a conversation\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if self.model.startswith(\"PetrosStav\"):\n",
    "            self.system_message = \"YOU MUST DELIMIT TOOL CALLS WITH <tool_call>...</tool_call> tags\" + system_message\n",
    "        else: \n",
    "            self.system_message = system_message\n",
    "        self.max_steps = max_steps\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "        self.tool_names = {tool[\"function\"][\"name\"] for tool in tools}\n",
    "\n",
    "    def run_agent(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Run the agent with the given user input.\n",
    "\n",
    "        Args:\n",
    "            user_input: The user's input message\n",
    "\n",
    "        Returns:\n",
    "            The final response from the agent\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=messages,\n",
    "                    tools=self.tools,\n",
    "                )\n",
    "\n",
    "                message = response.choices[0].message\n",
    "\n",
    "                print(f\"\\n🤖 Step {step + 1}: \\033[36m{message.content.rstrip()}\\033[0m\")\n",
    "\n",
    "                if step > 0 and is_done(message):\n",
    "                    print(f\"✨ \\033[32mRequest fulfilled after {step + 1} steps.\\033[0m\")\n",
    "                    return message.content\n",
    "                \n",
    "                if not message.tool_calls:\n",
    "                    message.tool_calls = [try_to_parse_tool_call(message)]\n",
    "\n",
    "                messages.append(message)\n",
    "\n",
    "                if not message.tool_calls:\n",
    "                    print(\"⚠️ \\033[33mNo tool calls found\\033[0m\")\n",
    "                    continue\n",
    "\n",
    "                for tool_call in message.tool_calls:\n",
    "                    if tool_call.function.name in self.tool_names:\n",
    "                        try:\n",
    "                            command = convert_tool_call_to_command(tool_call)\n",
    "\n",
    "                            print(f\"\\n🔧 \\033[35mExecuting command:\\033[0m \\033[1m{command}\\033[0m\")\n",
    "                            tool_result = execute_command(command)\n",
    "                            print(f\"📝 \\033[34mTOOL RESULT:\\033[0m {tool_result}\")\n",
    "\n",
    "                            messages.append(\n",
    "                                {\n",
    "                                    \"role\": \"tool\",\n",
    "                                    \"tool_call_id\": tool_call.id,\n",
    "                                    \"name\": tool_call.function.name,\n",
    "                                    \"content\": tool_result,\n",
    "                                }\n",
    "                            )\n",
    "                        except json.JSONDecodeError:\n",
    "                            error_msg = \"❌ \\033[31mError: Invalid JSON in tool arguments\\033[0m\"\n",
    "                            messages.append(\n",
    "                                {\n",
    "                                    \"role\": \"tool\",\n",
    "                                    \"tool_call_id\": tool_call.id,\n",
    "                                    \"name\": tool_call.function.name,\n",
    "                                    \"content\": error_msg,\n",
    "                                }\n",
    "                            )\n",
    "                    else:\n",
    "                        print(f\"⚠️ \\033[33mUnknown tool call {tool_call.function.name}. Skipping.\\033[0m\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ \\033[31mError during parsing  message. Message:\\n {message.content}\\nError\\n{str(e)}\\033[0m\")\n",
    "                return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "        print(\n",
    "            f\"⚠️ \\033[33mReached maximum number of steps ({self.max_steps}). Request may not be completely fulfilled.\\033[0m\"\n",
    "        )\n",
    "        return messages[-1].content"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b66a4b66096f8c42",
   "metadata": {},
   "source": [
    "## Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "id": "6187d43a11ccdd19",
   "metadata": {},
   "source": [
    "agent = CLIAgent(\n",
    "    max_steps=10,\n",
    "    client=client,\n",
    "    tools=[command_tool_description],\n",
    "    system_message=SYSTEM_MESSAGE,\n",
    "    model=MODEL_NAME,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"> \")\n",
    "\n",
    "        if not user_input or user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Exiting CLI Agent. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        agent.run_agent(user_input)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting CLI Agent. Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        raise e"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50356f06261a77f7",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "## MCP"
   ]
  },
  {
   "cell_type": "code",
   "id": "5123f916b72971f3",
   "metadata": {},
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "WORK_DIR = \".\"\n",
    "\n",
    "server_parameters = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"python-filesystem-mcp.py\", WORK_DIR],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_parameters) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "\n",
    "        tool_result = await session.list_tools()\n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": {\n",
    "                        k: v\n",
    "                        for k, v in tool.inputSchema.items()\n",
    "                        if k not in [\"additionalProperties\", \"$schema\"]\n",
    "                    },\n",
    "                }}\n",
    "            for tool in tool_result.tools\n",
    "        ]\n",
    "\n",
    "        print(\"\\n\\033[1;36m=== 🤖 MCP Agent ===\\033[0m\")\n",
    "        print(\"\\033[1;33mAvailable tools:\\033[0m\")\n",
    "        for tool in tool_result.tools:\n",
    "            print(f\"\\033[1;32m•\\033[0m {tool.name}: {tool.description}\")\n",
    "\n",
    "        messages = []\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"> \")\n",
    "\n",
    "                if not user_input or user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                    print(\"\\n\\033[1;36mExiting MCP Agent. Goodbye! 👋\\033[0m\")\n",
    "                    break\n",
    "\n",
    "                messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=messages,\n",
    "                    tools=tools,\n",
    "                    tool_choice=\"auto\"\n",
    "                )\n",
    "\n",
    "                choice = response.choices[0]\n",
    "\n",
    "                if not choice.message.tool_calls:\n",
    "                    choice.message.tool_calls = [try_to_parse_tool_call(choice.message)]\n",
    "                    if choice.message.tool_calls:\n",
    "                        choice.finish_reason = \"tool_calls\"\n",
    "\n",
    "                if choice.finish_reason != \"tool_calls\":\n",
    "                    output = choice.message.content\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "                    print(f\"\\n\\033[1;34m🤖 Assistant:\\033[0m {output}\")\n",
    "                    continue\n",
    "\n",
    "                tool_call = choice.message.tool_calls[0]\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                print(f\"\\n\\033[1;33m🛠️  Using tool:\\033[0m {tool_name}\")\n",
    "                print(f\"\\033[1;33m📝 Arguments:\\033[0m {json.dumps(tool_args, indent=2)}\")\n",
    "\n",
    "                result = await session.call_tool(\n",
    "                    name=tool_name,\n",
    "                    arguments=tool_args\n",
    "                )\n",
    "\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"tool_calls\": [tool_call.model_dump()]\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": str(result)\n",
    "                })\n",
    "\n",
    "                followup = client.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=messages\n",
    "                )\n",
    "                final = followup.choices[0].message.content\n",
    "                messages.append({\"role\": \"assistant\", \"content\": final})\n",
    "                print(f\"\\n\\033[1;34m🤖 Agent:\\033[0m {final}\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\033[1;36mExiting MCP Agent. Goodbye! 👋\\033[0m\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n\\033[1;31m❌ Error:\\033[0m {e}\")\n",
    "                break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d739ce271e2ff5",
   "metadata": {},
   "source": [
    "## Extras for self study\n",
    "- [Awesome MCP servers](https://mcpservers.org)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai)\n",
    "- [AI Agents course from Huggingface](https://huggingface.co/learn/agents-course/unit0/introduction)\n",
    "- [Koog: framework for writing agents in Kotlin](https://docs.koog.ai)\n",
    "- [Agent Developement Kit](https://google.github.io/adk-docs/)\n",
    "- [Smolagents](https://huggingface.co/docs/smolagents/index)\n",
    "- [Langgraph](https://langchain-ai.github.io/langgraph/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
